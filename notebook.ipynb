{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CHALLENGE 2022 : House Price Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy as sp\n",
    "#from metrics.custom_metric_ilb import custom_metric_function\n",
    "from pycaret.regression import compare_models, setup\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows=28\n",
    "pd.options.display.max_columns=28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA PREPROCESSING :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='data/'\n",
    "X_train_raw=pd.read_csv(filepath +'X_train_J01Z4CN.csv') \n",
    "Y_train_raw=pd.read_csv(filepath + 'y_train_OXxrJt1.csv')\n",
    "Y_test_raw=pd.read_csv(filepath + 'y_random_MhJDhKK.csv')\n",
    "X_test_raw=pd.read_csv(filepath + 'X_test_BEhvxAN.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0=X_train_raw.drop(columns=\"id_annonce\")\n",
    "X_test_0=X_test_raw.drop(columns=\"id_annonce\")\n",
    "X_train_0.head()\n",
    "X_test_ids=X_test_raw[\"id_annonce\"]\n",
    "X_test_ids.to_pickle(\"data/X_test_ids.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_0.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_0=Y_train_raw.drop(columns=\"id_annonce\")\n",
    "Y_train_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=[\"property_type\",\"city\",\"energy_performance_category\",\"ghg_category\",\"exposition\"]\n",
    "# for x in L:\n",
    "#     X_train[x]=X_train[x].astype(\"category\")\n",
    "#     X_test[x]=X_test[x].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0.isna().sum()/len(X_train_0.index)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(X_train_0.isnull(),cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning the data\n",
    "### a. DataTypes :\n",
    "First let's make sure that all the data has the proper type (especially that categorical data isn't set as numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical type Columns\n",
    "X_train_0.select_dtypes(np.number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0.select_dtypes(np.number).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0.select_dtypes(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0.select_dtypes(object).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Dealing with NaN Values :\n",
    "\n",
    "Missing values :\n",
    "\n",
    "```\n",
    "size                             512 # Numerical\n",
    "floor                          27625 # Numerical\n",
    "land_size                      21787 # Numerical\n",
    "energy_performance_value       18300 # Numerical\n",
    "energy_performance_category    18300 # Categorical\n",
    "ghg_value                      18838 # Numerical\n",
    "ghg_category                   18838 # Categorical\n",
    "exposition                     28274 # Categorical\n",
    "nb_rooms                        1566 # Numerical\n",
    "nb_bedrooms                     2733 # Numerical\n",
    "nb_bathrooms                   13273 # Numerical\n",
    "```\n",
    "\n",
    "- For houses we'll set the the floor value at 0\n",
    "- For appartements we'll set the landsize value at 0\n",
    "\n",
    "Based on the missing values heatmap we'll drop : \n",
    "- Energy__performance_category (Derived Data from Energy_performance value)\n",
    "- GHG_category (Derived Data from ghg_value)\n",
    "- Exposition (75.66% missing Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns\n",
    "data = pd.concat([X_train_0, X_test_0], axis=0).reset_index(drop=True)\n",
    "data_1=data.drop(columns=[\"exposition\",\"city\", \"energy_performance_category\", \"ghg_category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#KNN imputation / Try and expirement different imputations\n",
    "def knn_impute(df0, column):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    # Creating a copy of the input dataframe\n",
    "    df = df0.copy()\n",
    "\n",
    "    # numeric_df : subset of df composed only of numerical data type colums\n",
    "    numeric_df = df.select_dtypes(np.number)\n",
    "\n",
    "    # full columns : columns that have no missing data\n",
    "    full_columns=numeric_df.loc[:,numeric_df.isna().sum()==0].columns\n",
    "\n",
    "    # knn_x_train : training data for the missing values\n",
    "    knn_x_train = numeric_df.loc[numeric_df[column].isna()==False, full_columns]\n",
    "\n",
    "    # knn_y_train: target data for the missing valies \n",
    "    knn_y_train= numeric_df.loc[numeric_df[column].isna()==False, column]\n",
    "\n",
    "    # knn_x_test : the data with missing values for the target column\n",
    "    knn_x_test = numeric_df.loc[numeric_df[column].isna()==True, full_columns]\n",
    "\n",
    "    # Creating the KNeighbors Regress\n",
    "    knn=KNeighborsRegressor()\n",
    "\n",
    "    # Fitting the model\n",
    "    knn.fit(knn_x_train, knn_y_train)\n",
    "\n",
    "    y_pred=knn.predict(knn_x_test)\n",
    "\n",
    "    df.loc[df[column].isna()==True, column]=y_pred\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_impute_all(df, list_columns):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for column in list_columns:\n",
    "        df=knn_impute(df,column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = [\"size\", \"land_size\",\"energy_performance_value\",\"ghg_value\", \"nb_rooms\",\"nb_bathrooms\", \"nb_bedrooms\"]\n",
    "data_2 = knn_impute_all(data_1, list_columns=list_columns)\n",
    "data_2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.loc[(data_2['property_type']!=\"appartement\"), 'floor'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3=knn_impute(data_2, \"floor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Transformations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data_3.select_dtypes(np.number).columns :\n",
    "    print(f\"{column} : {skew(data_3[column])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding\n",
    "Issue 1 : Hot encoding city column : explostion in dimensionality (for now I dropped it )\n",
    "possible solution : Frequency encoding / Target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4 = pd.get_dummies(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(data_4)\n",
    "data_5=pd.DataFrame(scaler.transform(data_4), index=data_4.index, columns=data_4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_0.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sb.distplot(Y_train_0, kde=True, fit=sp.stats.norm)\n",
    "plt.title(\"Without Log Transform\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sb.distplot(np.log(Y_train_0), kde=True, fit=sp.stats.norm)\n",
    "plt.xlabel(\"Log SalePrice\")\n",
    "plt.title(\"With Log Transform\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_1=np.log(Y_train_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1=data_5.loc[:X_train_0.index.max(),:]\n",
    "X_train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1=data_5.loc[X_train_0.index.max()+1:,:]\n",
    "X_test_1.head()\n",
    "X_test_1.to_pickle(\"data/X_test_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_6=pd.concat([X_train_1, Y_train_1], axis=1)\n",
    "data_6.to_pickle(\"data/data.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from metrics.custom_metric_ilb import custom_metric_function\n",
    "from pycaret.regression import compare_models, setup\n",
    "import pycaret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup(feature_interaction=True,polynomial_features=True,use_gpu = True, silent = True, data = data_6, target=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model\n",
    "baseline_model=XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "             early_stopping_rounds=None, enable_categorical=False,\n",
    "             eval_metric=None, feature_types=None, gamma=0, gpu_id=0,\n",
    "             grow_policy='depthwise', importance_type=None,\n",
    "             interaction_constraints='', learning_rate=0.300000012, max_bin=256,\n",
    "             max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
    "             max_depth=6, max_leaves=0, min_child_weight=1,\n",
    "             monotone_constraints='()', n_estimators=100, n_jobs=-1,\n",
    "             num_parallel_tree=1, objective='reg:squarederror',\n",
    "             predictor='auto')\n",
    "baseline_model.fit(X_train_1, Y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(sklearn.metrics.SCORERS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model evaluation\n",
    "kf = KFold(n_splits=10)\n",
    "import sklearn\n",
    "baseline_result = cross_val_score(baseline_model, X_train_1, Y_train_1, scoring=\"neg_mean_absolute_error\", cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_baseline_result = -np.mean(baseline_result)\n",
    "print(mean_baseline_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"xgboost\":XGBRegressor(),\n",
    "    # \"lgbm\":LGBMRegressor(),\n",
    "    \"rfr\":RandomForestRegressor(),\n",
    "    \"etr\":ExtraTreesRegressor(),\n",
    "    # \"gbr\":GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_1, Y_train_1)\n",
    "    print(f\"Finished Training {model_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "\n",
    "for model_name, model in models.items() :\n",
    "    result = cross_val_score(model, X_train_1, Y_train_1, scoring=\"neg_mean_absolute_error\", cv=kf)\n",
    "    results[model_name]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name}. Mean : {-np.mean(result)}  & Var : {-np.std(result)} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_raw = (\n",
    "    1/3*np.exp(models[\"xgboost\"].predict(X_test_1)) +\n",
    "    # 0.2*np.exp(models[\"lgbm\"].predict(X_test_1)) +\n",
    "    1/3*np.exp(models[\"rfr\"].predict(X_test_1)) +\n",
    "    1/3*np.exp(models[\"etr\"].predict(X_test_1)) \n",
    "    # 0.2*np.exp(models[\"gbr\"].predict(X_test_1)) \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feauture selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.Series(np.exp(baseline_model.predict(X_test_1)), name=\"price\")\n",
    "final_predictions=pd.Series(final_predictions_raw,name=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.concat([X_test_ids, predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"data/submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = pd.concat([X_test_ids, final_predictions], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission.to_csv(\"data/final_submission.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('data-challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cad879572fc86dfe65b3e43b153c561e79ca25c5ffe8434d36e9a73cfcff219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
